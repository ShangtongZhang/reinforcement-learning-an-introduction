{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# 2017 Ji Yang(jyang7@ualberta.ca)                                    #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "\"\"\"\n",
    "Simulation of applying the in-place version of iterative policy evaluation algorithm in\n",
    "the grid world, from Sutton & Barto's RL Book, Section 4.1 Policy Evaluation (Prediction)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "WORLD_SIZE = 4\n",
    "REWARD = -1.0  # a constant reward for moving to non-terminal states\n",
    "ACTION_PROB = 0.25  # random policy\n",
    "GAMMA = 1.0  # episode task, no discount\n",
    "\n",
    "world = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "\n",
    "# left, up, right, down\n",
    "actions = ['L', 'U', 'R', 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid cell (col, row): 0 0 {'R': [0, 1], 'D': [1, 0], 'L': [0, 0], 'U': [0, 0]}\n",
      "Grid cell (col, row): 0 1 {'R': [0, 2], 'D': [1, 1], 'L': [0, 0], 'U': [0, 1]}\n",
      "Grid cell (col, row): 0 2 {'R': [0, 3], 'D': [1, 2], 'L': [0, 1], 'U': [0, 2]}\n",
      "Grid cell (col, row): 0 3 {'R': [0, 3], 'D': [1, 3], 'L': [0, 2], 'U': [0, 3]}\n",
      "Grid cell (col, row): 1 0 {'R': [1, 1], 'D': [2, 0], 'L': [1, 0], 'U': [0, 0]}\n",
      "Grid cell (col, row): 1 1 {'R': [1, 2], 'D': [2, 1], 'L': [1, 0], 'U': [0, 1]}\n",
      "Grid cell (col, row): 1 2 {'R': [1, 3], 'D': [2, 2], 'L': [1, 1], 'U': [0, 2]}\n",
      "Grid cell (col, row): 1 3 {'R': [1, 3], 'D': [2, 3], 'L': [1, 2], 'U': [0, 3]}\n",
      "Grid cell (col, row): 2 0 {'R': [2, 1], 'D': [3, 0], 'L': [2, 0], 'U': [1, 0]}\n",
      "Grid cell (col, row): 2 1 {'R': [2, 2], 'D': [3, 1], 'L': [2, 0], 'U': [1, 1]}\n",
      "Grid cell (col, row): 2 2 {'R': [2, 3], 'D': [3, 2], 'L': [2, 1], 'U': [1, 2]}\n",
      "Grid cell (col, row): 2 3 {'R': [2, 3], 'D': [3, 3], 'L': [2, 2], 'U': [1, 3]}\n",
      "Grid cell (col, row): 3 0 {'R': [3, 1], 'D': [3, 0], 'L': [3, 0], 'U': [2, 0]}\n",
      "Grid cell (col, row): 3 1 {'R': [3, 2], 'D': [3, 1], 'L': [3, 0], 'U': [2, 1]}\n",
      "Grid cell (col, row): 3 2 {'R': [3, 3], 'D': [3, 2], 'L': [3, 1], 'U': [2, 2]}\n",
      "Grid cell (col, row): 3 3 {'R': [3, 3], 'D': [3, 3], 'L': [3, 2], 'U': [2, 3]}\n"
     ]
    }
   ],
   "source": [
    "# here we generate the possible next state for each cell in the grid world\n",
    "next_state = []\n",
    "for i in range(0, WORLD_SIZE):\n",
    "    next_state.append([])\n",
    "    for j in range(0, WORLD_SIZE):\n",
    "        next = dict()\n",
    "        # uncomment this print and the last print in this loop if you haven't got it\n",
    "        print('Grid cell (col, row):', i, j, end=' ')\n",
    "\n",
    "        if i == 0:\n",
    "            next['U'] = [i, j]\n",
    "        else:\n",
    "            next['U'] = [i - 1, j]\n",
    "\n",
    "        if i == WORLD_SIZE - 1:\n",
    "            next['D'] = [i, j]\n",
    "        else:\n",
    "            next['D'] = [i + 1, j]\n",
    "\n",
    "        if j == 0:\n",
    "            next['L'] = [i, j]\n",
    "        else:\n",
    "            next['L'] = [i, j - 1]\n",
    "\n",
    "        if j == WORLD_SIZE - 1:\n",
    "            next['R'] = [i, j]\n",
    "        else:\n",
    "            next['R'] = [i, j + 1]\n",
    "\n",
    "        print(next)\n",
    "\n",
    "        next_state[i].append(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [0, 2], [0, 3], [1, 0], [1, 1], [1, 2], [1, 3], [2, 0], [2, 1], [2, 2], [2, 3], [3, 0], [3, 1], [3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# generate state coordinates in the grid word, except for the terminal state\n",
    "states = []\n",
    "for i in range(0, WORLD_SIZE):\n",
    "    for j in range(0, WORLD_SIZE):\n",
    "        # handle the top-left and bottom-right terminal state\n",
    "        if (i == 0 and j == 0) or (i == WORLD_SIZE - 1 and j == WORLD_SIZE - 1):\n",
    "            continue\n",
    "        else:\n",
    "            states.append([i, j])\n",
    "\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "k = 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "k = 2\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "k = 3\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "k = 10\n",
      "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n",
      "Random Policy\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Figure 4.1\n",
    "k = 0\n",
    "figure_plot_ks = [0, 1, 2, 3, 10]\n",
    "threshold_delta = 1e-9  # a very small positive number\n",
    "\n",
    "# keep iterating until convergence\n",
    "while True:\n",
    "    new_world = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    if k in figure_plot_ks:\n",
    "        print('k = {}'.format(k))\n",
    "        print(world)\n",
    "    k += 1\n",
    "    for i, j in states:\n",
    "        for action in actions:\n",
    "            new_state = next_state[i][j][action]\n",
    "            # expected update by the Bellman equation\n",
    "            new_world[i, j] += ACTION_PROB * GAMMA * (REWARD + world[new_state[0], new_state[1]])\n",
    "\n",
    "    # convergence check\n",
    "    if np.sum(np.abs(world - new_world)) < threshold_delta:\n",
    "        print('Random Policy')\n",
    "        print(new_world)\n",
    "        break\n",
    "    world = new_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
